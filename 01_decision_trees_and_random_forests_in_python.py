# -*- coding: utf-8 -*-
"""01-Decision Trees and Random Forests in Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZIN5OMp1yFGYkuVg2BpgbHs0SaPcnveT

___

<a href='http://www.pieriandata.com'> <img src='../Pierian_Data_Logo.png' /></a>
___


## Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

"""## Get the Data"""

df = pd.read_csv('kyphosis.csv')

df.head()

"""## EDA

We'll just check out a simple pairplot for this small dataset.

It seems like you are trying to create a pairplot using Seaborn's pairplot function. However, you haven't provided the dataframe (df) you want to use for the pairplot. Additionally, you've specified the hue parameter as 'Kyphosis' and the palette parameter as 'Set1', indicating that you want to differentiate the data points based on the 'Kyphosis' column and use the 'Set1' color palette.
In data visualization, the hue parameter is used to add an additional categorical variable to a plot, allowing you to differentiate data points based on that variable. When you specify a column name or variable as the hue parameter, the plot will assign different colors or visual markers to the data points corresponding to each unique value of that column.

For example, if you have a dataset with a column named 'Species' that contains different types of flowers (e.g., 'Setosa', 'Versicolor', 'Virginica'), you can use the hue parameter to visualize the relationship between multiple numerical variables while color-coding the data points by flower species. Each unique flower species will be represented by a different color in the plot, making it easier to identify patterns or differences between the groups.

The hue parameter is commonly used in various seaborn plots, such as scatter plots, bar plots, box plots, and pair plots, to provide additional visual information and facilitate comparisons between different groups or categories within the data.
"""

sns.pairplot(df,hue='Kyphosis',palette='Set1')

"""## Train Test Split

Let's split up the data into a training set and a test set!
"""

from sklearn.model_selection import train_test_split

X = df.drop('Kyphosis',axis=1)
y = df['Kyphosis']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

"""## Decision Trees

We'll start just by training a single decision tree.
"""

from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier()

dtree.fit(X_train,y_train)

"""## Prediction and Evaluation

Let's evaluate our decision tree.
"""

predictions = dtree.predict(X_test)

from sklearn.metrics import classification_report,confusion_matrix

print(classification_report(y_test,predictions))

print(confusion_matrix(y_test,predictions))

"""## Tree Visualization

Scikit learn actually has some built-in visualization capabilities for decision trees, you won't use this often and it requires you to install the pydot library, but here is an example of what it looks like and the code to execute this:
"""

from IPython.display import Image
from sklearn.externals.six import StringIO
from sklearn.tree import export_graphviz
import pydot

features = list(df.columns[1:])
features

dot_data = StringIO()
export_graphviz(dtree, out_file=dot_data,feature_names=features,filled=True,rounded=True)

graph = pydot.graph_from_dot_data(dot_data.getvalue())
Image(graph[0].create_png())

"""## Random Forests

Now let's compare the decision tree model to a random forest.
"""

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier(n_estimators=100)
rfc.fit(X_train, y_train)

rfc_pred = rfc.predict(X_test)

print(confusion_matrix(y_test,rfc_pred))

print(classification_report(y_test,rfc_pred))

